{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import os.path as osp\n",
    "import json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from utils_copy import *\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit import RDConfig\n",
    "from collections import OrderedDict\n",
    "from rdkit.Chem import ChemicalFeatures\n",
    "fdef_name = osp.join(RDConfig.RDDataDir, 'BaseFeatures.fdef')\n",
    "chem_feature_factory = ChemicalFeatures.BuildFeatureFactory(fdef_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PROTEIN = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y',\n",
    "                'X']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv('dataset/'+ filename)\n",
    "    drugs, prots = list(df['compound_iso_smiles']),list(df['target_sequence'])\n",
    "    return drugs, prots\n",
    "\n",
    "\n",
    "def dic_normalize(dic):\n",
    "    max_value = dic[max(dic, key=dic.get)]\n",
    "    min_value = dic[min(dic, key=dic.get)]\n",
    "    # print(max_value)\n",
    "    interval = float(max_value) - float(min_value)\n",
    "    for key in dic.keys():\n",
    "        dic[key] = (dic[key] - min_value) / interval\n",
    "    dic['X'] = (max_value + min_value) / 2.0\n",
    "    return dic\n",
    "\n",
    "seq_dict = {v:(i+1) for i,v in enumerate(VOCAB_PROTEIN)}\n",
    "pro_res_aliphatic_table = ['A', 'I', 'L', 'M', 'V']\n",
    "pro_res_aromatic_table = ['F', 'W', 'Y']\n",
    "pro_res_polar_neutral_table = ['C', 'N', 'Q', 'S', 'T']\n",
    "pro_res_acidic_charged_table = ['D', 'E']\n",
    "pro_res_basic_charged_table = ['H', 'K', 'R']\n",
    "\n",
    "res_weight_table = {'A': 71.08, 'C': 103.15, 'D': 115.09, 'E': 129.12, 'F': 147.18, 'G': 57.05, 'H': 137.14,\n",
    "                    'I': 113.16, 'K': 128.18, 'L': 113.16, 'M': 131.20, 'N': 114.11, 'P': 97.12, 'Q': 128.13,\n",
    "                    'R': 156.19, 'S': 87.08, 'T': 101.11, 'V': 99.13, 'W': 186.22, 'Y': 163.18}\n",
    "res_weight_table['X'] = np.average([res_weight_table[k] for k in res_weight_table.keys()])\n",
    "\n",
    "res_pka_table = {'A': 2.34, 'C': 1.96, 'D': 1.88, 'E': 2.19, 'F': 1.83, 'G': 2.34, 'H': 1.82, 'I': 2.36,\n",
    "                 'K': 2.18, 'L': 2.36, 'M': 2.28, 'N': 2.02, 'P': 1.99, 'Q': 2.17, 'R': 2.17, 'S': 2.21,\n",
    "                 'T': 2.09, 'V': 2.32, 'W': 2.83, 'Y': 2.32}\n",
    "res_pka_table['X'] = np.average([res_pka_table[k] for k in res_pka_table.keys()])\n",
    "\n",
    "res_pkb_table = {'A': 9.69, 'C': 10.28, 'D': 9.60, 'E': 9.67, 'F': 9.13, 'G': 9.60, 'H': 9.17,\n",
    "                 'I': 9.60, 'K': 8.95, 'L': 9.60, 'M': 9.21, 'N': 8.80, 'P': 10.60, 'Q': 9.13,\n",
    "                 'R': 9.04, 'S': 9.15, 'T': 9.10, 'V': 9.62, 'W': 9.39, 'Y': 9.62}\n",
    "res_pkb_table['X'] = np.average([res_pkb_table[k] for k in res_pkb_table.keys()])\n",
    "\n",
    "res_pkx_table = {'A': 0.00, 'C': 8.18, 'D': 3.65, 'E': 4.25, 'F': 0.00, 'G': 0, 'H': 6.00,\n",
    "                 'I': 0.00, 'K': 10.53, 'L': 0.00, 'M': 0.00, 'N': 0.00, 'P': 0.00, 'Q': 0.00,\n",
    "                 'R': 12.48, 'S': 0.00, 'T': 0.00, 'V': 0.00, 'W': 0.00, 'Y': 0.00}\n",
    "res_pkx_table['X'] = np.average([res_pkx_table[k] for k in res_pkx_table.keys()])\n",
    "\n",
    "res_pl_table = {'A': 6.00, 'C': 5.07, 'D': 2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, 'H': 7.59,\n",
    "                'I': 6.02, 'K': 9.74, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.30, 'Q': 5.65,\n",
    "                'R': 10.76, 'S': 5.68, 'T': 5.60, 'V': 5.96, 'W': 5.89, 'Y': 5.96}\n",
    "res_pl_table['X'] = np.average([res_pl_table[k] for k in res_pl_table.keys()])\n",
    "\n",
    "res_hydrophobic_ph2_table = {'A': 47, 'C': 52, 'D': -18, 'E': 8, 'F': 92, 'G': 0, 'H': -42, 'I': 100,\n",
    "                             'K': -37, 'L': 100, 'M': 74, 'N': -41, 'P': -46, 'Q': -18, 'R': -26, 'S': -7,\n",
    "                             'T': 13, 'V': 79, 'W': 84, 'Y': 49}\n",
    "res_hydrophobic_ph2_table['X'] = np.average([res_hydrophobic_ph2_table[k] for k in res_hydrophobic_ph2_table.keys()])\n",
    "\n",
    "res_hydrophobic_ph7_table = {'A': 41, 'C': 49, 'D': -55, 'E': -31, 'F': 100, 'G': 0, 'H': 8, 'I': 99,\n",
    "                             'K': -23, 'L': 97, 'M': 74, 'N': -28, 'P': -46, 'Q': -10, 'R': -14, 'S': -5,\n",
    "                             'T': 13, 'V': 76, 'W': 97, 'Y': 63}\n",
    "res_hydrophobic_ph7_table['X'] = np.average([res_hydrophobic_ph7_table[k] for k in res_hydrophobic_ph7_table.keys()])\n",
    "\n",
    "# nomarlize the residue feature\n",
    "res_weight_table = dic_normalize(res_weight_table)\n",
    "res_pka_table = dic_normalize(res_pka_table)\n",
    "res_pkb_table = dic_normalize(res_pkb_table)\n",
    "res_pkx_table = dic_normalize(res_pkx_table)\n",
    "res_pl_table = dic_normalize(res_pl_table)\n",
    "res_hydrophobic_ph2_table = dic_normalize(res_hydrophobic_ph2_table)\n",
    "res_hydrophobic_ph7_table = dic_normalize(res_hydrophobic_ph7_table)\n",
    "\n",
    "def one_hot_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "# one ont encoding with unknown symbol\n",
    "def one_hot_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def encoding_unk(x, allowable_set):\n",
    "    list = [False for i in range(len(allowable_set))]\n",
    "    i = 0\n",
    "    for atom in x:\n",
    "        if atom in allowable_set:\n",
    "            list[allowable_set.index(atom)] = True\n",
    "            i += 1\n",
    "    if i != len(x):\n",
    "        list[-1] = True\n",
    "    return list\n",
    "\n",
    "def residue_features(residue):\n",
    "    res_property1 = [1 if residue in pro_res_aliphatic_table else 0, 1 if residue in pro_res_aromatic_table else 0,\n",
    "                     1 if residue in pro_res_polar_neutral_table else 0,\n",
    "                     1 if residue in pro_res_acidic_charged_table else 0,\n",
    "                     1 if residue in pro_res_basic_charged_table else 0]\n",
    "    res_property2 = [res_weight_table[residue], res_pka_table[residue], res_pkb_table[residue], res_pkx_table[residue],\n",
    "                     res_pl_table[residue], res_hydrophobic_ph2_table[residue], res_hydrophobic_ph7_table[residue]]\n",
    "\n",
    "    return np.array(res_property1 + res_property2)\n",
    "\n",
    "# target feature for target graph\n",
    "def PSSM_calculation(aln_file, pro_seq):\n",
    "    pfm_mat = np.zeros((len(VOCAB_PROTEIN), len(pro_seq)))\n",
    "    with open(aln_file, 'r') as f:\n",
    "        line_count = len(f.readlines())\n",
    "        for line in f.readlines():\n",
    "            if len(line) != len(pro_seq):\n",
    "                print('error', len(line), len(pro_seq))\n",
    "                continue\n",
    "            count = 0\n",
    "            for res in line:\n",
    "                if res not in VOCAB_PROTEIN:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                pfm_mat[VOCAB_PROTEIN.index(res), count] += 1\n",
    "                count += 1\n",
    "\n",
    "    pseudocount = 0.8\n",
    "    ppm_mat = (pfm_mat + pseudocount / 4) / (float(line_count) + pseudocount)\n",
    "    pssm_mat = ppm_mat\n",
    "\n",
    "    return pssm_mat\n",
    "\n",
    "def seq_feature(pro_seq):\n",
    "    pro_hot = np.zeros((len(pro_seq), len(VOCAB_PROTEIN)))\n",
    "    pro_property = np.zeros((len(pro_seq), 12))\n",
    "    for i in range(len(pro_seq)):\n",
    "        # if 'X' in pro_seq:\n",
    "        #     print(pro_seq)\n",
    "        pro_hot[i,] = one_hot_encoding(pro_seq[i], VOCAB_PROTEIN)\n",
    "        pro_property[i,] = residue_features(pro_seq[i])\n",
    "    return np.concatenate((pro_hot, pro_property), axis=1)\n",
    "\n",
    "def target_feature(aln_file, pro_seq):\n",
    "    pssm = PSSM_calculation(aln_file, pro_seq)\n",
    "    other_feature = seq_feature(pro_seq)\n",
    "\n",
    "    return np.concatenate((np.transpose(pssm, (1, 0)), other_feature), axis=1)\n",
    "\n",
    "def target2feature(target_key, target_sequence, aln_dir):\n",
    "    aln_file = os.path.join(aln_dir, target_key + '.aln')\n",
    "    feature = target_feature(aln_file, target_sequence)\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# target sequence to target graph\n",
    "def sequence2graph(target_key, target_sequence, distance_dir):\n",
    "    target_edge_index = []\n",
    "    target_edge_distance = []\n",
    "    target_size = len(target_sequence)\n",
    "\n",
    "    target_feature = seq_feature(target_sequence)\n",
    "    contact_map_file = os.path.join(distance_dir, target_key + '.npy')\n",
    "    distance_map = np.load(contact_map_file)\n",
    "\n",
    "    for i in range(target_size):\n",
    "        distance_map[i, i] = 1\n",
    "        if i + 1 < target_size:\n",
    "            distance_map[i, i + 1] = 1\n",
    "    index_row, index_col = np.where(distance_map >= 0.5)  # for threshold\n",
    "\n",
    "    for i, j in zip(index_row, index_col):\n",
    "        target_edge_index.append([i, j])  # dege\n",
    "        target_edge_distance.append(distance_map[i, j])  # edge weight\n",
    "\n",
    "    target_feature = torch.Tensor(target_feature)\n",
    "    target_edge_index = torch.LongTensor(target_edge_index).transpose(1, 0)\n",
    "    target_edge_distance = torch.FloatTensor(target_edge_distance)\n",
    "\n",
    "    return target_size, target_feature, target_edge_index, target_edge_distance\n",
    "\n",
    "def get_nodes(g):\n",
    "    feat = []\n",
    "    for n, d in g.nodes(data=True):\n",
    "        h_t = []\n",
    "        h_t += [int(d['a_type'] == x) for x in ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na',\n",
    "                                                'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb',\n",
    "                                                'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H', 'Li',\n",
    "                                                'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
    "                                                'Pt', 'Hg', 'Pb', 'X']]\n",
    "        h_t.append(d['a_num'])\n",
    "        h_t.append(d['acceptor'])\n",
    "        h_t.append(d['donor'])\n",
    "        h_t.append(int(d['aromatic']))\n",
    "        h_t += [int(d['degree'] == x) for x in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "        h_t += [int(d['ImplicitValence'] == x) for x in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "        h_t += [int(d['num_h'] == x) for x in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "        h_t += [int(d['hybridization'] == x) for x in (Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3)]\n",
    "        # 5 more\n",
    "        h_t.append(d['ExplicitValence'])\n",
    "        h_t.append(d['FormalCharge'])\n",
    "        h_t.append(d['NumExplicitHs'])\n",
    "        h_t.append(d['NumRadicalElectrons'])\n",
    "        feat.append((n, h_t))\n",
    "    feat.sort(key=lambda item: item[0])\n",
    "    node_attr = torch.FloatTensor([item[1] for item in feat])\n",
    "    return node_attr\n",
    "\n",
    "def get_edges(g):\n",
    "    e = {}\n",
    "    for n1, n2, d in g.edges(data=True):\n",
    "        e_t = [int(d['b_type'] == x)\n",
    "                for x in (Chem.rdchem.BondType.SINGLE, \\\n",
    "                            Chem.rdchem.BondType.DOUBLE, \\\n",
    "                            Chem.rdchem.BondType.TRIPLE, \\\n",
    "                            Chem.rdchem.BondType.AROMATIC)]\n",
    "\n",
    "        e_t.append(int(d['IsConjugated'] == False))\n",
    "        e_t.append(int(d['IsConjugated'] == True))\n",
    "        e[(n1, n2)] = e_t\n",
    "\n",
    "    edge_index = torch.LongTensor(list(e.keys())).transpose(0, 1)\n",
    "    edge_attr = torch.FloatTensor(list(e.values()))\n",
    "\n",
    "\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "# mol smile to mol graph edge index\n",
    "def smile2graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "\n",
    "    feats = chem_feature_factory.GetFeaturesForMol(mol)\n",
    "    mol_size = mol.GetNumAtoms()\n",
    "    g = nx.DiGraph()\n",
    "    \n",
    "    # Create nodes\n",
    "    for i in range(mol.GetNumAtoms()):\n",
    "        atom_i = mol.GetAtomWithIdx(i)\n",
    "        g.add_node(i,\n",
    "                a_type=atom_i.GetSymbol(),\n",
    "                a_num=atom_i.GetAtomicNum(),\n",
    "                acceptor=0,\n",
    "                donor=0,\n",
    "                aromatic=atom_i.GetIsAromatic(),\n",
    "                hybridization=atom_i.GetHybridization(),\n",
    "                num_h=atom_i.GetTotalNumHs(),\n",
    "                degree = atom_i.GetDegree(),\n",
    "                # 5 more node features\n",
    "                ExplicitValence=atom_i.GetExplicitValence(),\n",
    "                FormalCharge=atom_i.GetFormalCharge(),\n",
    "                ImplicitValence=atom_i.GetImplicitValence(),\n",
    "                NumExplicitHs=atom_i.GetNumExplicitHs(),\n",
    "                NumRadicalElectrons=atom_i.GetNumRadicalElectrons(),\n",
    "            )\n",
    "            \n",
    "    for i in range(len(feats)):\n",
    "        if feats[i].GetFamily() == 'Donor':\n",
    "            node_list = feats[i].GetAtomIds()\n",
    "            for n in node_list:\n",
    "                g.nodes[n]['donor'] = 1\n",
    "        elif feats[i].GetFamily() == 'Acceptor':\n",
    "            node_list = feats[i].GetAtomIds()\n",
    "            for n in node_list:\n",
    "                g.nodes[n]['acceptor']\n",
    "    # Read Edges\n",
    "    for i in range(mol.GetNumAtoms()):\n",
    "        for j in range(mol.GetNumAtoms()):\n",
    "            e_ij = mol.GetBondBetweenAtoms(i, j)\n",
    "            if e_ij is not None:\n",
    "                g.add_edge(i, j,\n",
    "                            b_type=e_ij.GetBondType(),\n",
    "                            # 1 more edge features 2 dim\n",
    "                            IsConjugated=int(e_ij.GetIsConjugated()),\n",
    "                            )\n",
    "                \n",
    "    node_attr = get_nodes(g)\n",
    "    edge_index, edge_attr = get_edges(g)         \n",
    "\n",
    "    return mol_size, node_attr, edge_index, edge_attr\n",
    "\n",
    "def create_dataset(dataset):\n",
    "    dataset_dir = os.path.join('dataset',dataset)\n",
    "    # drug smiles\n",
    "    ligands = json.load(open(os.path.join(dataset_dir, 'ligands_can.txt')), object_pairs_hook=OrderedDict)\n",
    "    # protein sequences\n",
    "    proteins = json.load(open(os.path.join(dataset_dir, 'proteins.txt')), object_pairs_hook=OrderedDict)\n",
    "\n",
    "    # load protein feature and predicted distance map\n",
    "    process_dir = os.path.join('dataset/')\n",
    "    pro_distance_dir = os.path.join(process_dir, dataset, 'pconsc4')  # numpy .npy file\n",
    "    \n",
    "    # dataset process\n",
    "    drugs = []  # rdkit entity\n",
    "    prots = []  # sequences\n",
    "    prot_keys = []  # protein id (or name)\n",
    "    drug_smiles = []  # smiles\n",
    "    # create molecule graph\n",
    "    print(\"create molecule graph ...\")\n",
    "    # smiles\n",
    "    for d in ligands.keys():\n",
    "        lg = ligands[d]\n",
    "        drugs.append(lg)\n",
    "        drug_smiles.append(ligands[d])\n",
    "        smile_graph = {}\n",
    "\n",
    "    for i in tqdm(range(len(drugs))):\n",
    "        smile = drugs[i]\n",
    "\n",
    "        g_d = smile2graph(smile)\n",
    "        smile_graph[smile] = g_d\n",
    "\n",
    "    print(\"create protein graph ...\")  \n",
    "    # seqs\n",
    "    for t in proteins.keys():\n",
    "        prots.append(proteins[t])\n",
    "        prot_keys.append(t)  \n",
    "\n",
    "    target_graph = {}\n",
    "    for i in tqdm(range(len(prot_keys))):\n",
    "        key = prot_keys[i]\n",
    "        protein = prots[i]\n",
    "        g_t = sequence2graph(key, protein, pro_distance_dir)\n",
    "        target_graph[protein] = g_t\n",
    "    \n",
    "    # read files(train and test)\n",
    "    test_csv = dataset + '/raw/test.csv'\n",
    "    test_drugs, test_prots = read_data(test_csv)\n",
    "    test_drugs, test_prots = np.asarray(test_drugs), np.asarray(test_prots)\n",
    "\n",
    "    test_data = DTADataset(root='dataset', dataset=dataset + '_' + 'test', drug_smiles=test_drugs, target_sequence=test_prots,smile_graph=smile_graph, target_graph=target_graph)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PROTEIN = { \"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \n",
    "\t\t\t\t\"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10, \"M\": 11, \"N\": 12, \n",
    "\t\t\t\t\"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \n",
    "\t\t\t\t\"W\": 19, \"Y\": 20, \"X\": 21}\n",
    "\n",
    "def PROTEIN2INT(target):\n",
    "    return [VOCAB_PROTEIN[s] for s in target] \n",
    "\n",
    "# initialize the dataset\n",
    "\n",
    "class DTADataset(InMemoryDataset):\n",
    "    def __init__(self, root='data', dataset='davis',\n",
    "                drug_smiles=None, target_sequence=None, x=None, x_mask=None, xt=None, xt_mask=None, y=None, transform=None,\n",
    "                pre_transform=None, smile_graph=None,  target_graph=None):\n",
    "        super(DTADataset, self).__init__(root, transform, pre_transform)\n",
    "        self.dataset = dataset\n",
    "        self.drug_smiles = drug_smiles\n",
    "        self.target_sequence = target_sequence\n",
    "        self.y = y\n",
    "        self.smile_graph = smile_graph\n",
    "        self.target_graph = target_graph\n",
    "\n",
    "        self.process(drug_smiles, target_sequence, x, x_mask, xt, xt_mask, smile_graph, target_graph)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.dataset + '_data_mol.pt', self.dataset + '_data_pro.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def _download(self):\n",
    "        pass\n",
    "\n",
    "    def _process(self):\n",
    "        if not os.path.exists(self.processed_dir):\n",
    "            os.makedirs(self.processed_dir)\n",
    "\n",
    "    def process(self, drug_smiles=None, target_sequence=None, x=None, x_mask=None, xt=None, xt_mask=None, smile_graph=None, target_graph=None):\n",
    "        assert (len(drug_smiles) == len(target_sequence)), 'The three lists must have the same length!'\n",
    "        data_list_mol = []\n",
    "        data_list_pro = []\n",
    "\n",
    "        data_len = len(drug_smiles)\n",
    "        print('loading tensors ...')\n",
    "        for i in tqdm(range(data_len)):\n",
    "\n",
    "            smiles = drug_smiles[i]\n",
    "            tar_seq = target_sequence[i]\n",
    "            if x is not None:\n",
    "                drug = x[i]\n",
    "            if x_mask is not None:\n",
    "                drug_mask = x_mask[i]\n",
    "            if xt is not None:\n",
    "                target = xt[i]\n",
    "            if xt_mask is not None:\n",
    "                target_mask = xt_mask[i]\n",
    "\n",
    "            mol_size, mol_features, mol_edge_index, mol_edge_attr= smile_graph[smiles]\n",
    "            target_size, target_features, target_edge_index, target_edge_weight = target_graph[tar_seq]\n",
    "            \n",
    "            GCNData_mol = DATA.Data(x=mol_features,\n",
    "                                    edge_index=mol_edge_index,\n",
    "                                    edge_attr = mol_edge_attr,)\n",
    "            if x is not None:\n",
    "                GCNData_mol.drug = torch.LongTensor([drug])\n",
    "            if x_mask is not None:\n",
    "                GCNData_mol.drug_mask = torch.LongTensor([drug_mask])\n",
    "            GCNData_mol.__setitem__('c_size', torch.LongTensor([mol_size]))\n",
    "\n",
    "            target_seq = PROTEIN2INT(tar_seq)\n",
    "            target_seq_len = 1200\n",
    "            if len(target_seq) < target_seq_len:\n",
    "                pro_seq_emb = np.pad(target_seq, (0, target_seq_len- len(target_seq)))\n",
    "            else:\n",
    "                pro_seq_emb = target_seq[:target_seq_len]\n",
    "\n",
    "            GCNData_pro = DATA.Data(x=target_features,\n",
    "                                    edge_index=target_edge_index,\n",
    "                                    edge_attr = target_edge_weight,\n",
    "                                    pro_emb = torch.LongTensor([pro_seq_emb]),)\n",
    "            \n",
    "            if xt is not None:\n",
    "                GCNData_pro.target = torch.LongTensor([target])\n",
    "            if xt_mask is not None:\n",
    "                GCNData_pro.target_mask = torch.LongTensor([target_mask])\n",
    "            GCNData_pro.__setitem__('target_size', torch.LongTensor([target_size]))\n",
    "\n",
    "            data_list_mol.append(GCNData_mol)\n",
    "            data_list_pro.append(GCNData_pro)\n",
    "\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list_mol = [data for data in data_list_mol if self.pre_filter(data)]\n",
    "            data_list_pro = [data for data in data_list_pro if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list_mol = [self.pre_transform(data) for data in data_list_mol]\n",
    "            data_list_pro = [self.pre_transform(data) for data in data_list_pro]\n",
    "\n",
    "\n",
    "        self.data_mol = data_list_mol\n",
    "        self.data_pro = data_list_pro\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drug_smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return GNNData_mol, GNNData_pro\n",
    "        return self.data_mol[idx], self.data_pro[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Any, Dict, Optional\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_geometric.nn import GPSConv,GCNConv,GATConv,GINConv,global_max_pool as gmp, global_mean_pool as gep\n",
    "from collections import OrderedDict\n",
    "from torch_geometric.nn.resolver import normalization_resolver\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from torch.nn import (\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    "    Dropout\n",
    ")\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=64):\n",
    "        super(Attention, self).__init__()\n",
    "        self.project_x = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "        self.project_xt = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "        self.project_st = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "    def forward(self, x, xt,st):\n",
    "        x = self.project_x(x)\n",
    "        xt = self.project_xt(xt)\n",
    "        st = self.project_st(st)\n",
    "        a = torch.cat((x, xt, st), 1)\n",
    "        a = torch.softmax(a, dim=1)\n",
    "        return a\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channel:int, local_conv: Optional[MessagePassing], heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.local_conv = local_conv\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.attn = nn.MultiheadAttention(in_channel, heads, batch_first= True)\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_channel, in_channel * 2),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(in_channel * 2, in_channel),\n",
    "            Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = normalization_resolver('batch_norm', in_channel)\n",
    "        self.norm2 = normalization_resolver('batch_norm', in_channel)\n",
    "        self.norm3 = normalization_resolver('batch_norm', in_channel)\n",
    "        \n",
    "    def forward(self, x: Tensor,  edge_index: Adj, batch: Optional[torch.Tensor] = None)->Tensor:\n",
    "        \n",
    "        hs = []\n",
    "        h = self.local_conv(x,edge_index)\n",
    "        h = F.dropout(h, p=self.dropout, training= self.training)\n",
    "        h = h + x\n",
    "        h = self.norm1(h)\n",
    "        hs.append(h)\n",
    "        \n",
    "        # Global attention transformer-style model.\n",
    "        h, mask = to_dense_batch(x, batch)\n",
    "        h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
    "        h = h[mask]\n",
    "        h = F.dropout(h,p=self.dropout, training= self.training)\n",
    "        h = h + x\n",
    "        h = self.norm2(h)\n",
    "        hs.append(h)\n",
    "        \n",
    "        # Combine local and global outputs.\n",
    "        out = sum(hs)\n",
    "        out = out + self.mlp(out)\n",
    "        out = self.norm3(out)\n",
    "        \n",
    "        return out    \n",
    "\n",
    "class molGraphRepresentation(nn.Module):\n",
    "    def __init__(self, node_dim, embedding_dim, num_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.convs = ModuleList()\n",
    "        self.node_linear = Sequential(\n",
    "            Linear(node_dim,embedding_dim),\n",
    "            ReLU(),\n",
    "            Linear(embedding_dim,embedding_dim),\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(embedding_dim, embedding_dim * 2),\n",
    "                ReLU(),\n",
    "                Linear(embedding_dim * 2, embedding_dim),\n",
    "            )\n",
    "            conv = GPSConv(embedding_dim, GINConv(nn), heads=4, dropout=dropout)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.global_fc = Sequential(\n",
    "            Linear(embedding_dim, 1024),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(1024, embedding_dim),\n",
    "            #Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,data):\n",
    "        data.x = self.node_linear(data.x)\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, batch)\n",
    "        x = gep(x,batch)\n",
    "        x = self.global_fc(x)\n",
    "        return x\n",
    "\n",
    "class proGraphRepresentation(nn.Module):\n",
    "    def __init__(self, num_features_pro,dropout,output_dim):\n",
    "        super().__init__()\n",
    "        self.pro_conv = nn.ModuleList([])\n",
    "        self.pro_conv.append(GCNConv(num_features_pro, num_features_pro * 4))\n",
    "        self.pro_conv.append(GATConv(num_features_pro * 4, num_features_pro * 4, heads=4, dropout=dropout, concat=False))\n",
    "        self.pro_conv.append(GATConv(num_features_pro * 4, num_features_pro * 4, heads=4, dropout=dropout, concat=False))\n",
    "        self.pro_out_feats = num_features_pro * 4\n",
    "        self.pro_seq_fc1 = nn.Linear(num_features_pro * 4, num_features_pro * 4)\n",
    "        self.pro_seq_fc2 = nn.Linear(num_features_pro * 4, num_features_pro * 4)\n",
    "        self.pro_bias = nn.Parameter(torch.rand(1, num_features_pro * 4))\n",
    "        torch.nn.init.uniform_(self.pro_bias, a=-0.2, b=0.2)\n",
    "        self.global_fc = Sequential(\n",
    "            Linear(num_features_pro * 4, 1024),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(1024, output_dim),\n",
    "            Dropout(dropout)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,data):\n",
    "        # get protein input\n",
    "        x, edge_index, weight, batch = data.x, data.edge_index, data.edge_weight, data.batch\n",
    "        n = x.size(0)\n",
    "        for i in range(len(self.pro_conv)):\n",
    "            if i == 0:\n",
    "                xc = self.pro_conv[i](x, edge_index, weight)\n",
    "            else:\n",
    "                xc = self.pro_conv[i](x, edge_index)\n",
    "            if i < len(self.pro_conv) - 1:\n",
    "                xc = self.relu(xc)\n",
    "            if i == 0:\n",
    "                x = xc\n",
    "                continue\n",
    "            pro_z = torch.sigmoid(\n",
    "                self.pro_seq_fc1(xc) + self.pro_seq_fc2(x) + self.pro_bias.expand(n, self.pro_out_feats))\n",
    "            x = pro_z * xc + (1 - pro_z) * x\n",
    "\n",
    "        x = gmp(x, batch)  # global pooling\n",
    "        # flatten\n",
    "        x = self.global_fc(x)\n",
    "        return x\n",
    "class Conv1dReLU(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.inc = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inc(x)\n",
    "\n",
    "\n",
    "class LinearReLU(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.inc = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=out_features, bias=bias),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inc(x)\n",
    "\n",
    "\n",
    "class StackCNN(nn.Module):\n",
    "    def __init__(self, layer_num, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inc = nn.Sequential(OrderedDict([('conv_layer0',\n",
    "                                               Conv1dReLU(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                                                          stride=stride, padding=padding))]))\n",
    "        for layer_idx in range(layer_num - 1):\n",
    "            self.inc.add_module('conv_layer%d' % (layer_idx + 1),\n",
    "                                Conv1dReLU(out_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                           padding=padding))\n",
    "\n",
    "        self.inc.add_module('pool_layer', nn.AdaptiveMaxPool1d(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inc(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class proSequenceRePresentation(nn.Module):\n",
    "    def __init__(self, block_num, vocab_size, embedding_num):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_num, padding_idx=0)\n",
    "        self.block_list = nn.ModuleList()\n",
    "        for block_idx in range(block_num):\n",
    "            self.block_list.append(\n",
    "                StackCNN(block_idx + 1, embedding_num, 96, 3)\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Linear(3 * 96, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).permute(0, 2, 1)\n",
    "        feats = [block(x) for block in self.block_list]\n",
    "\n",
    "        x = torch.cat(feats, -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MMSGDTA(torch.nn.Module):\n",
    "    def __init__(self,num_features_pro=33, num_features_mol=88, embed_dim=128, dropout=0.2):\n",
    "        super(MMSGDTA, self).__init__()\n",
    "\n",
    "        print('MMSGDTA Loading ...')\n",
    "        \n",
    "        self.ligand_encoder = molGraphRepresentation(num_features_mol,num_layers = 4, embedding_dim=embed_dim,dropout=dropout)\n",
    "        self.progra_encoder = proGraphRepresentation(num_features_pro,dropout,embed_dim)\n",
    "        self.proseq_encoder = proSequenceRePresentation(block_num=3,vocab_size = 22, embedding_num=128)\n",
    "        \n",
    "        self.attention = Attention(embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(3 * embed_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        data_pro_seq = data_pro.pro_emb\n",
    "        mol_x = self.ligand_encoder(data_mol)\n",
    "        pro_x = self.progra_encoder(data_pro)\n",
    "        pro_s = self.proseq_encoder(data_pro_seq)\n",
    "        \n",
    "        a = self.attention(mol_x, pro_x, pro_s)\n",
    "        emb = torch.stack([mol_x, pro_x, pro_s], dim=1)\n",
    "        a = a.unsqueeze(dim=2)\n",
    "        fused_emb = (a * emb).reshape(-1, 3 * 128)\n",
    "\n",
    "        xc = self.fc1(fused_emb)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSGDTA Loading ...\n",
      "create molecule graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create protein graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tensors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1329.41it/s]\n",
      "/data10/XJH/anaconda3/envs/PCSG/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'test'\n",
    "model_file_name = 'Your trained weights file'\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = MMSGDTA().to(device)\n",
    "test_data = create_dataset(dataset)\n",
    "test_loader = DataLoader(test_data, batch_size= 512, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device,dataloader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    #total_labels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(dataloader.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            data_mol = data[0].to(device)\n",
    "            data_pro = data[1].to(device)\n",
    "\n",
    "            output = model(data_mol, data_pro)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "    \n",
    "    return total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305764/910732913.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file_name,map_location=torch.device('cpu')),strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make prediction for 1 samples...\n",
      "[10.328574]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(model_file_name):\n",
    "    model.load_state_dict(torch.load(model_file_name,map_location=torch.device('cpu')),strict=False)\n",
    "    P = predicting(model,device,test_loader)\n",
    "\n",
    "    print(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
